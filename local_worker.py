import os
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

os.environ["NO_PROXY"] = "localhost,127.0.0.1"

# é…ç½®ï¼šæ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– Ollama åœ°å€ (Docker éƒ¨ç½²æ—¶ä½¿ç”¨)
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://127.0.0.1:11434")
EMBED_MODEL = "nomic-embed-text"
LOCAL_LLM = "qwen2.5:7b"

def get_retriever(db_path):
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®æ•°æ®åº“è·¯å¾„ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„æ£€ç´¢å™¨
    """
    print(f"ğŸ”Œ [Local Worker] æ­£åœ¨è¿æ¥çŸ¥è¯†åº“: {db_path}")
    embeddings = OllamaEmbeddings(
        model=EMBED_MODEL,
        base_url=OLLAMA_BASE_URL
    )
    vectorstore = Chroma(
        persist_directory=db_path, 
        embedding_function=embeddings
    )
    # æ‰©å¤§æœç´¢èŒƒå›´åˆ° 10
    return vectorstore.as_retriever(search_kwargs={"k": 10})

def get_grader_chain():
    """è¿”å›è¯„åˆ†é“¾å¯¹è±¡ (æ— çŠ¶æ€ï¼Œå¯å¤ç”¨)"""
    llm = ChatOllama(
        model=LOCAL_LLM, 
        temperature=0, 
        format="json",
        base_url=OLLAMA_BASE_URL
    )

    prompt = ChatPromptTemplate.from_template(
        """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ–‡æ¡£è¯„åˆ†å‘˜ã€‚
        è¯·è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µæ˜¯å¦ä¸ç”¨æˆ·çš„é—®é¢˜ç›¸å…³ã€‚
        å¦‚æœæ–‡æ¡£åŒ…å«ç›¸å…³å…³é”®è¯æˆ–è¯­ä¹‰ï¼Œè¯„åˆ†ä¸º 'yes'ï¼Œå¦åˆ™ä¸º 'no'ã€‚
        å¿…é¡»è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼š
        {{ "score": "yes" }} æˆ– {{ "score": "no" }}

        é—®é¢˜: {question}
        æ–‡æ¡£: {document}
        JSON è¾“å‡º:
        """
    )
    return prompt | llm | JsonOutputParser()