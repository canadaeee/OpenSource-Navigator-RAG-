import os
# ç»§ç»­ä¿æŒä»£ç†ç™½åå•ï¼Œé˜²æ­¢ Kimi è¿æ¥å—é˜»
os.environ["NO_PROXY"] = "localhost,127.0.0.1"

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# --- é…ç½® ---
# ä½¿ç”¨ Kimi (Moonshot)
CLOUD_LLM_MODEL = "moonshot-v1-8k" 

def build_generator_chain():
    """
    æ„å»ºç”Ÿæˆå™¨é“¾ï¼šContext + Question -> Answer
    """
    print("ğŸ§  åˆå§‹åŒ–äº‘ç«¯å¤§è„‘ (Kimi Generator)...")
    
    llm = ChatOpenAI(
        model=CLOUD_LLM_MODEL,
        temperature=0.3, # ç¨å¾®æœ‰ç‚¹æ¸©åº¦ï¼Œè®©å›ç­”è‡ªç„¶äº›
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE")
    )

    prompt = ChatPromptTemplate.from_template(
        """ä½ æ˜¯ä¸€ä¸ªç²¾é€š Python å’Œå¼€æºé¡¹ç›®çš„ AI æ¶æ„å¸ˆã€‚
        ä½ å°†æ ¹æ®æä¾›çš„ã€ä¸Šä¸‹æ–‡ä»£ç ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
        
        å¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯šå®åœ°è¯´â€œæˆ‘åœ¨æä¾›çš„ä»£ç ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯â€ï¼Œä¸è¦ç¼–é€ ã€‚
        è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼Œå¹¶å°½å¯èƒ½å¼•ç”¨ä»£ç ä¸­çš„å‡½æ•°åæˆ–å˜é‡åã€‚

        ---
        ã€ä¸Šä¸‹æ–‡ä»£ç ã€‘:
        {context}
        ---
        ã€ç”¨æˆ·é—®é¢˜ã€‘:
        {question}
        ---
        ã€ä½ çš„å›ç­”ã€‘:
        """
    )

    # æ„é€ é“¾: Prompt -> LLM -> Stringè¾“å‡º
    chain = prompt | llm | StrOutputParser()
    return chain

# å®ä¾‹åŒ–
generator_chain = build_generator_chain()

if __name__ == "__main__":
    print("ğŸš€ Cloud Brain ç‹¬ç«‹æµ‹è¯•")
    
    # --- æ¨¡æ‹Ÿä¸€ä¸ªæµ‹è¯•åœºæ™¯ ---
    # å‡è®¾ Local Worker å·²ç»æ‰¾åˆ°äº†è¿™ä¸¤ä¸ªç‰‡æ®µä¼ ç»™æˆ‘ä»¬
    mock_context = """
    File: config.py
    class Config:
        def __init__(self):
            # ç”¨æˆ·éœ€è¦åœ¨è¿™é‡Œå¡«å…¥ API Key
            self.api_key = os.getenv("GLM_API_KEY", "")
            self.base_url = "https://open.bigmodel.cn/api/paas/v4/"
    """
    
    mock_question = "æˆ‘åº”è¯¥åœ¨ç¯å¢ƒå˜é‡é‡Œå«ä»€ä¹ˆåå­—æ¥é…ç½® API Keyï¼Ÿ"
    
    print(f"\nâ“ æ¨¡æ‹Ÿæé—®: {mock_question}")
    print(f"ğŸ“„ æ¨¡æ‹Ÿä¸Šä¸‹æ–‡: (åŒ…å« Config ç±»å’Œ GLM_API_KEY)")
    print("-" * 30)
    
    try:
        print("ğŸ’¡ Kimi æ­£åœ¨æ€è€ƒ...")
        response = generator_chain.invoke({
            "context": mock_context,
            "question": mock_question
        })
        print(f"\nâœ… Kimi å›ç­”:\n{response}")
    except Exception as e:
        print(f"âŒ Kimi è°ƒç”¨å¤±è´¥: {e}")
        