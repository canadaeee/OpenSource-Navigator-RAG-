import os
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ä¿æŒä»£ç†ç™½åå•
os.environ["NO_PROXY"] = "localhost,127.0.0.1"

load_dotenv()

# é¡¹ç›®æ ¹ç›®å½•é…ç½®
SOURCE_ROOT = "source_code"
DB_ROOT = "chroma_db_store"

def get_readme_content(project_name):
    """
    å°è¯•è¯»å–é¡¹ç›®çš„ README æ–‡ä»¶
    """
    project_path = os.path.join(SOURCE_ROOT, project_name)
    
    # å¸¸è§çš„ README æ–‡ä»¶å
    possible_names = ["README.md", "readme.md", "README.rst", "README.txt", "README_en.md"]
    
    for name in possible_names:
        file_path = os.path.join(project_path, name)
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    # åªè¯»å–å‰ 3000 å­—ç¬¦ï¼Œæ—¢çœé’±åˆèƒ½è¦†ç›–æ ¸å¿ƒä»‹ç»
                    return f.read()[:3000]
            except Exception:
                continue
    return None

def generate_suggestions(project_name):
    """
    æ ¸å¿ƒåŠŸèƒ½ï¼šç”Ÿæˆå»ºè®®é—®é¢˜
    1. å…ˆæ£€æŸ¥æœ‰æ²¡æœ‰ç¼“å­˜çš„ questions.json
    2. å¦‚æœæ²¡æœ‰ï¼Œè¯»å– README å¹¶è°ƒç”¨ Kimi ç”Ÿæˆ
    3. ä¿å­˜ç¼“å­˜å¹¶è¿”å›
    """
    
    # 1. æ£€æŸ¥ç¼“å­˜
    cache_path = os.path.join(DB_ROOT, project_name, "questions.json")
    
    if os.path.exists(cache_path):
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                print(f"âš¡ [Profiler] åŠ è½½ç¼“å­˜çš„å»ºè®®é—®é¢˜: {project_name}")
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ [Profiler] ç¼“å­˜è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")

    # 2. è·å– README å†…å®¹
    print(f"ğŸ§  [Profiler] æ­£åœ¨åˆ†æé¡¹ç›®æ–‡æ¡£ä»¥ç”Ÿæˆå»ºè®®...")
    readme_content = get_readme_content(project_name)
    
    if not readme_content:
        # å¦‚æœè¿ README éƒ½æ²¡æœ‰ï¼Œè¿”å›é€šç”¨é—®é¢˜
        return ["å¦‚ä½•å®‰è£…ä¾èµ–ï¼Ÿ", "é¡¹ç›®çš„ä¸»å…¥å£æ–‡ä»¶æ˜¯å“ªä¸ªï¼Ÿ", "å¦‚ä½•è¿è¡Œæµ‹è¯•ï¼Ÿ"]

    # 3. è°ƒç”¨ Cloud Brain (Kimi)
    try:
        llm = ChatOpenAI(
            model="moonshot-v1-8k",
            temperature=0.5, # ç¨å¾®é«˜ä¸€ç‚¹ï¼Œå¢åŠ åˆ›é€ æ€§
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )

        prompt = ChatPromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„å¼€æºé¡¹ç›®åˆ†æå¸ˆã€‚
            è¯·æ ¹æ®ä»¥ä¸‹çš„é¡¹ç›® README å†…å®¹ï¼Œä¸ºå¼€å‘è€…æå‡º 4 ä¸ªæœ€æœ‰ä»·å€¼çš„å…¥é—¨æŠ€æœ¯é—®é¢˜ã€‚
            
            é—®é¢˜åº”è¯¥å…³æ³¨ï¼šå®‰è£…é…ç½®ã€æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨ã€æ¶æ„é€»è¾‘æˆ–éƒ¨ç½²æ–¹å¼ã€‚
            è¯·ç›´æ¥è¾“å‡ºé—®é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦å¸¦åºå·ï¼Œä¸è¦å¸¦å…¶ä»–åºŸè¯ã€‚
            
            ---
            README æ‘˜è¦:
            {context}
            ---
            å»ºè®®é—®é¢˜åˆ—è¡¨:
            """
        )

        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({"context": readme_content})
        
        # å¤„ç†ç»“æœï¼šæŒ‰è¡Œåˆ†å‰²ï¼Œè¿‡æ»¤ç©ºè¡Œ
        questions = [q.strip().replace("- ", "").replace("1. ", "") for q in result.split("\n") if q.strip()]
        # å–å‰ 4 ä¸ª
        questions = questions[:4]

        # 4. å†™å…¥ç¼“å­˜
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(questions, f, ensure_ascii=False)
            
        return questions

    except Exception as e:
        print(f"âŒ [Profiler] ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
        return ["è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ", "å¦‚ä½•å¿«é€Ÿå¼€å§‹ï¼Ÿ"]
        