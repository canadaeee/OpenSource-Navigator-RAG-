import os
# ==========================================
# ğŸ›¡ï¸ å…³é”®ä¿®å¤ï¼šé˜²æ­¢ VPN/ä»£ç† æ‹¦æˆªæœ¬åœ°æµé‡
# ==========================================
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
# ==========================================

import time
import requests
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

load_dotenv()

def test_cloud_brain():
    """æµ‹è¯•äº‘ç«¯å¤§è„‘ (Kimi)"""
    print("ğŸŒ [1/3] æ­£åœ¨å‘¼å«äº‘ç«¯å¤§è„‘ (Kimi)...")
    try:
        llm = ChatOpenAI(
            model="moonshot-v1-8k",
            temperature=0.3,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        response = llm.invoke([HumanMessage(content="Hello Kimi.")])
        print(f"âœ… Kimi å“åº”æˆåŠŸ: {response.content}")
        return True
    except Exception as e:
        print(f"âŒ Kimi è¿æ¥å¤±è´¥: {e}")
        return False

def debug_ollama_raw():
    """åº•å±‚æ¥å£è¯Šæ–­"""
    print("\nğŸ” [2/3] æ­£åœ¨è¿›è¡Œ Ollama åº•å±‚æ¥å£è¯Šæ–­...")
    try:
        url = "http://127.0.0.1:11434/api/generate"
        payload = {"model": "qwen2.5:7b", "prompt": "hi", "stream": False}
        # ä¿æŒ 30ç§’ è¶…æ—¶
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code == 200:
            print("âœ… Ollama åº•å±‚æ¥å£é€šç•… (HTTP 200)")
            return True
        else:
            print(f"âŒ Ollama æ¥å£æŠ¥é”™: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Ollama åº•å±‚è¿æ¥å¤±è´¥: {e}")
        return False

def test_local_worker():
    """æµ‹è¯•æœ¬åœ°å·¥å…µ (LangChain Wrapper)"""
    print("\nğŸ  [3/3] æ­£åœ¨å‘¼å«æœ¬åœ°å·¥å…µ (LangChain Wrapper)...")
    try:
        # è¿™é‡Œä¾ç„¶ä¿æŒæ˜¾å¼æŒ‡å®š base_url
        llm = ChatOllama(
            model="qwen2.5:7b",
            temperature=0.1,
            base_url="http://127.0.0.1:11434"
        )
        
        t0 = time.time()
        response = llm.invoke([HumanMessage(content="Ready?")])
        t1 = time.time()
        
        print(f"âœ… Local Worker å“åº”æˆåŠŸ: {response.content}")
        print(f"âš¡ è€—æ—¶: {t1-t0:.2f}ç§’")
        return True
    except Exception as e:
        print(f"âŒ Local Worker (LangChain) å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ OpenSource Navigator - æœ€ç»ˆè¿é€šæ€§æµ‹è¯•")
    print("="*50)
    
    cloud_ok = test_cloud_brain()
    raw_ok = debug_ollama_raw()
    
    if raw_ok:
        local_ok = test_local_worker()
    else:
        local_ok = False
    
    print("\n" + "="*50)
    if cloud_ok and local_ok:
        print("ğŸ‰ Phase 1 å®Œç¾é€šå…³ï¼æ··åˆæ¶æ„å·²å°±ç»ªã€‚")
    else:
        print("âš ï¸ ä»æœ‰æ•…éšœï¼Œè¯·ç»§ç»­åé¦ˆã€‚")